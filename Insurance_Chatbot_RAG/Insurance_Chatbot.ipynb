{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxT3ewMy72lvop4zrYSM1R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivsharanrupesh/ALL_RAG/blob/main/Insurance_Chatbot_RAG/Insurance_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG Evaluation with Langchain and RAGAS**\n",
        "\n",
        "In the following notebook I will be exploring the following:\n",
        "\n",
        "1. Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "2. Evaluating our pipeline with the Ragas library\n",
        "3. Making an adjustment to our RAG pipeline\n",
        "4. Evaluating our adjusted pipeline against our baseline"
      ],
      "metadata": {
        "id": "zM4XKLLr7Tha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X1gP_-rM6Y_s"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-openai langchain_core langchain-community langchainhub openai ragas tiktoken cohere faiss_cpu requests tokenizers pypdf2 unstructured langchain langchain_together\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  sentence_transformers  rank_bm25 > /dev/null"
      ],
      "metadata": {
        "id": "L6LtdRFh73sI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "BtyKkkYh8qHf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building our RAG pipeline**\n",
        "\n",
        "I will:\n",
        "\n",
        "1. Create an Index\n",
        "2. Use a LLM to generate responses based on the retrieved context\n",
        "\n",
        "Let's get started by creating our index.\n",
        "\n",
        "\n",
        "**Loading Data**"
      ],
      "metadata": {
        "id": "VPchVmea8umX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive_path = drive.mount('/content/MyDrive/')\n",
        "\n",
        "#!ls -R '/content/MyDrive/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxvCrY009QAa",
        "outputId": "3e5cdbb4-7f50-4b92-afa0-91ea3e19eac4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/MyDrive/; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "#Replace with the actual path to your Markdown file in Colab:\n",
        "markdown_file_path = '/content/MyDrive/MyDrive/dataset/policy-booklet-0923.md'\n",
        "loader = UnstructuredMarkdownLoader(markdown_file_path)\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "e5U96T6l8573"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iammpnBe_oKh",
        "outputId": "deaa02a3-d4f7-4c54-b0ac-ce3577b491bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '/content/MyDrive/MyDrive/dataset/policy-booklet-0923.md'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transforming Data**\n",
        "\n",
        "Now that I have gotten my single document - let's split it into smaller pieces so I can more effectively leverage it with our retrieval chain!\n",
        "\n",
        "We'll start with the classic: *RecursiveCharacterTextSplitter.*"
      ],
      "metadata": {
        "id": "xLLI5gF3_0Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "documents = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "DoLJF3fn_xq4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's confirm we have split our policy document."
      ],
      "metadata": {
        "id": "EqEP_BipAYVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcPlbgOwAX9a",
        "outputId": "0620f2ce-a219-4bc4-9716-17330c7de5df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading OpenAI Embeddings Model**\n",
        "\n",
        "We'll need a process by which we can convert our text into vectors that allow us to compare to our query vector.\n",
        "\n",
        "Let's use\n",
        " **OpenAI's text-embedding-ada-002**\n",
        " for this task! (soon we'll be able to leverage OpenAI's newest embedding model which is waiting on an approved PR to be merged as we speak!)"
      ],
      "metadata": {
        "id": "ZL2EVlQAAjh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20SW0GSbASSV",
        "outputId": "3d54dc79-4276-4aac-9250-820791f5b3d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e4a8b6e740bc>:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=api_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a FAISS VectorStore**\n",
        "\n",
        "Now that i have my documents - I'll need a place to store them alongside their embeddings.\n",
        "\n",
        "I will be using Meta's FAISS for this task."
      ],
      "metadata": {
        "id": "q1Ru0EJeA5xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "sWRnQnDEAp5x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Retriever**\n",
        "\n",
        "To complete my index, all that's left to do is expose my vectorstore as a retriever"
      ],
      "metadata": {
        "id": "cKb_DEwkBJGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "wIKylyiCBNTb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the Retriever**\n",
        "\n",
        "Now that we've gone through the trouble of creating our retriever - let's see it in action!"
      ],
      "metadata": {
        "id": "HaMk-5f1BRZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_documents = retriever.invoke(\"how much will you pay if my car is damaged?\")\n",
        "for doc in retrieved_documents:\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O4aKiqZBVRk",
        "outputId": "b19621cc-1d56-4bba-ece4-e67dcb4bb99f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faqs How Much Will You Pay If My Car Is Damaged?\n",
            "\n",
            "Where damage to your car is covered under your policy, we'll pay the cost of repairing or replacing your car up to its UK market value. This is the current value of your car at the time of the claim. It may be different to the amount you paid or any amount you provided when you insured your car with us.\n",
            "\n",
            "Who Is Covered To Drive Other Cars?\n",
            "\n",
            "Your certificate of motor insurance will show who has cover to drive other cars. We'll only cover injury to third parties, or damage caused to their property, not to the car being driven. See 'Section 1: Liability' on page 11. Am I covered if I leave my car unlocked or the keys in the car? We won't pay a claim for theft or attempted theft if your car is left:\n",
            "\n",
            "Unlocked.\n",
            "\n",
            "With keys or key fobs in, on, or attached to the car.\n",
            "\n",
            "With the engine running.\n",
            "\n",
            "With a window or roof open.\n",
            "\n",
            "What's not included in my cover?\n",
            "\n",
            "We don't cover things like:\n",
            "\n",
            "Mechanical or electrical failure.\n",
            "\n",
            "Wear and tear.\n",
            "A courtesy car may not be available on the day.\n",
            "\n",
            "You can only drive the courtesy car within the territorial limits. It cannot be used in the Republic of Ireland.\n",
            "\n",
            "Whilst You Have Your Courtesy Car\n",
            "\n",
            "If your insurance cover is third party, fire and theft, we'll also provide cover under sections 4 and 5 for the courtesy car. If you claim under these sections, you'll have to pay the first:\n",
            "\n",
            "£250 if you claim under section 4 for accidental damage.\n",
            "\n",
            "£115 if you claim under section 5 for windscreen replacement.\n",
            "\n",
            "£25 if you claim for windscreen repair.\n",
            "\n",
            "You'Re Not Covered For\n",
            "\n",
            "8 If your car is written off, or is stolen and not recovered, you won't get a courtesy car.\n",
            "\n",
            "Section 4: Accidental Damage\n",
            "\n",
            "Damage to your car We'll put things right if your car is damaged.\n",
            "\n",
            "Accidental Damage To Your Car Included With: Essential Comp Comp+\n",
            "\n",
            "If your car is accidentally damaged, we can choose to either:\n",
            "\n",
            "Repair - we'll repair the damage ourselves or pay to repair it.\n",
            "There are no hire cars available, and no alternative cars are available for hire.\n",
            "\n",
            "You can use this benefit any time in the 21 days following your claim. You'll need to pay the costs up front, and then send us your receipts or proof of travel. We can only pay you back once we receive these.\n",
            "\n",
            "If You'Re Outside The Territorial Limits\n",
            "\n",
            "If we're dealing with your claim under sections 2 or 4 of your policy and the loss or damage happens outside the territorial limits, we'll either:\n",
            "\n",
            "repay your travel costs up to £50 per day, up to a total of £500 per claim, or treat your claim as if it happened within the territorial limits so long as: - you can get your car back to the territorial limits for it to be repaired, or - your car is written off, or is stolen and not recovered.\n",
            "\n",
            "You'Re Not Covered For\n",
            "\n",
            "8 We won't provide you with a hire car if you're only claiming for windscreen or glass damage.\n",
            "\n",
            "Paying A Deposit\n",
            "When Am I Covered?\n",
            "\n",
            "If we're dealing with your claim under sections 2 or 4 of your policy, we'll arrange for a hire car company to provide you with a hire car when both of the following apply:\n",
            "\n",
            "Your car is damaged because of an accident, fire or theft, or if it's stolen and not recovered.\n",
            "\n",
            "The loss or damage happens within the territorial limits.\n",
            "\n",
            "For details of your cover in the Republic of Ireland, see 'Where you can drive' on page 31.\n",
            "\n",
            "How Much Am I Covered For?\n",
            "\n",
            "If your car can be repaired, and is driveable\n",
            "\n",
            "We'll provide you with a hire car from the point your car goes in for repair:\n",
            "\n",
            "If you use our approved repairer, until they've repaired your car.\n",
            "\n",
            "If you use your own repairer, for up to 21 days in a row while they're repairing your car. If your car can be repaired, and is not driveable\n",
            "\n",
            "As soon as you've confirmed that we can start the repair, we'll provide you with a hire car:\n",
            "\n",
            "If you use our approved repairer, until they have repaired your car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a RAG Chain**\n",
        "\n",
        "**Creating a Prompt Template**\n",
        "\n",
        "There are a few different ways I could create a prompt template - I could create a custom template, as seen in the code below, or I will simply pull a prompt from the prompt hub! Let's look at an example of that!"
      ],
      "metadata": {
        "id": "GU4_F2SUBxHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "retrieval_qa_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOTq5ZvjB03D",
        "outputId": "91b7e7e1-f417-4e30-8d6c-c87b84d8bb7b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieval_qa_prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piFRuopjCQ_9",
        "outputId": "485f73a9-284e-4621-f56a-cb77173b70fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer any use questions based solely on the context below:\n",
            "\n",
            "<context>\n",
            "{context}\n",
            "</context>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - the prompt template is simple - but we'll create our own to be a bit more specific!\n",
        "\n"
      ],
      "metadata": {
        "id": "yAEHLie3Cc2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "vcSgNCsPDBa1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up the Basic QA Chain**\n",
        "\n",
        "Now we can instantiate the basic RAG chain!\n",
        "\n",
        "I'll use LCEL directly just to see an example of it\n",
        "\n",
        "I'll also ensure to pass-through our context - which is critical for RAGAS."
      ],
      "metadata": {
        "id": "WIogWw0nDHDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "primary_qa_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "_gMd9KUpDGyJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "-UUa-EwqEOQ6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "gKxopN9eEWZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how much will you pay if my car is damaged?\"\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\": question})\n",
        "print(result[\"response\"].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_1K3jRHEXOH",
        "outputId": "845236ca-0193-4bfb-ca1a-ee3c557b6c9b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We'll pay the cost of repairing or replacing your car up to its UK market value.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Are my electric car's charging cables covered?\"\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\": question})\n",
        "print(result[\"response\"].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jum3JHjZEili",
        "outputId": "04802ee5-b0f0-4791-eaf7-426fea6e9ca4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your home charger and charging cables are considered an accessory to your car. This means they're covered under 'Section 2: Fire and theft' or 'Section 4: Accidental damage' of your policy.\n"
          ]
        }
      ]
    }
  ]
}